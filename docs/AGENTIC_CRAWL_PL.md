# Crawling Agentowy z Inteligentnym Wyborem Silnika

## PrzeglÄ…d

**Agentic Crawl** to zaawansowana funkcja KnowledgeTree, ktÃ³ra pozwala na automatyczne pobieranie i ekstrakcjÄ™ informacji ze stron internetowych z wykorzystaniem sztucznej inteligencji. System automatycznie wybiera optymalny silnik crawlingu na podstawie analizy URLi i zadania.

## ğŸ¯ Kluczowe FunkcjonalnoÅ›ci

### 1. **Inteligentny WybÃ³r Silnika**

System automatycznie wybiera najbardziej odpowiedni silnik crawlingu:

- **HTTP Scraper** (80% przypadkÃ³w)
  - Najszybszy silnik
  - Ideally dla statycznych stron HTML
  - Blogs, dokumentacja, Wikipedia, prosty e-commerce

- **Playwright** (15% przypadkÃ³w)
  - Renderowanie JavaScript w rzeczywistej przeglÄ…darce
  - Social media, SPA (Single Page Applications)
  - Strony z dynamicznÄ… treÅ›ciÄ…

- **Firecrawl** (5% przypadkÃ³w - opcjonalny)
  - Premium API dla trudnych stron
  - Anti-bot protection, heavy JavaScript
  - Wymaga klucza API (opcjonalny)

### 2. **Ekstrakcja Kierowana Promptem**

UÅ¼ytkownik podaje naturalny prompt w jÄ™zyku polskim lub angielskim, a AI ekstrauje dokÅ‚adnie te informacje, ktÃ³rych potrzeba.

**PrzykÅ‚adowe Prompty:**
```
"wyciÄ…gnij wszystkie firmy z nazwÄ…, adresem i danymi kontaktowymi"

"znajdÅº wszystkie informacje o metodykach konserwacji drewna"

"wejdÅº na film YouTube, wyciÄ…gnij transkrypcjÄ™ i zbuduj artykuÅ‚"

"zbierz wszystkie produkty z cenami i opisami"
```

### 3. **ObsÅ‚uga Wielu URLi**

- Jednorazowe przetwarzanie do 20 URLi
- Automatyczne Å‚Ä…czenie wynikÃ³w w hierarchiczne drzewo kategorii
- RÃ³wnolegÅ‚e przetwarzanie dla lepszej wydajnoÅ›ci

## ğŸš€ Jak UÅ¼ywaÄ‡

### Przez Interfejs Webowy

1. **PrzejdÅº do sekcji Dokumenty**
2. **Wybierz projekt**
3. **Kliknij "Crawling Agentowy" z ikonÄ… AI**
4. **Dodaj URLe**:
   - Wpisz jeden lub wiÄ™cej URLi (max 20)
   - MoÅ¼esz dodawaÄ‡ kolejne URLe przyciskiem "Dodaj URL"
   - UsuÅ„ niepotrzebne URLe klikajÄ…c (X)

5. **Napisz Prompt AI**:
   - Opisz co chcesz wyekstrahowaÄ‡ (min. 10 znakÃ³w)
   - UÅ¼ywaj jÄ™zyka naturalnego (polski lub angielski)
   - BÄ…dÅº konkretny - lepsze prompty = lepsze wyniki

6. **Rozpocznij Crawling**:
   - System automatycznie wybierze optymalny silnik
   - Przetwarzanie odbywa siÄ™ w tle (Celery)
   - Otrzymasz Job ID do Å›ledzenia postÄ™pu

### Przez API

```bash
curl -X POST "http://localhost:8765/api/v1/crawl/agentic" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "urls": [
      "https://example.com/company-list",
      "https://example.com/company-details"
    ],
    "agent_prompt": "wyciÄ…gnij wszystkie firmy z nazwÄ…, adresem, danymi kontaktowymi i stronÄ… internetowÄ…"
  }'
```

**OdpowiedÅº:**
```json
{
  "success": true,
  "job_id": 42,
  "message": "Agentic extraction started for 2 URLs with custom prompt",
  "agent_prompt": "wyciÄ…gnij wszystkie firmy...",
  "urls_count": 2,
  "note": "AI agents will extract structured information according to your prompt"
}
```

## ğŸ”§ Architektura Techniczna

### Backend (Python/FastAPI)

#### 1. **Endpoint API** (`backend/api/routes/crawl.py`)
```python
@router.post("/agentic")
async def crawl_with_agent_prompt(
    request: AgenticCrawlRequest,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Agentic Crawl with intelligent engine selection
    - Analyzes URLs and prompt
    - Automatically selects optimal engine
    - Creates CrawlJob and launches Celery task
    """
```

#### 2. **Celery Task** (`backend/services/document_tasks.py`)
```python
@celery_app.task(name="services.document_tasks.process_agentic_crawl_task")
def process_agentic_crawl_task(
    self,
    crawl_job_id: int,
    urls: List[str],
    agent_prompt: str,
    project_id: int,
    engine: Optional[str] = None,
    category_id: Optional[int] = None
) -> Dict[str, Any]:
    """
    Background task processing:
    1. Intelligent engine selection
    2. URL scraping
    3. AI-guided extraction
    4. Knowledge tree generation
    5. Document + chunks persistence
    """
```

#### 3. **Intelligent Engine Selector** (`backend/services/intelligent_crawler_selector.py`)
```python
class IntelligentCrawlerSelector:
    """
    Analyzes URLs and task prompt to automatically select
    optimal scraping engine (HTTP, Playwright, or Firecrawl)

    Decision factors:
    - URL patterns (domain, path, extensions)
    - Content type predictions
    - Prompt analysis (complexity, requirements)
    - Historical success rates
    """

    def select_engine(
        self,
        urls: List[str],
        prompt: str
    ) -> CrawlEngine:
        """Returns optimal engine with confidence score"""
```

### Frontend (React/TypeScript)

#### **AgenticCrawlDialog Component** (`frontend/src/components/AgenticCrawlDialog.tsx`)
```typescript
interface AgenticCrawlDialogProps {
  projectId: number;
  onSuccess?: (jobId: number) => void;
}

export function AgenticCrawlDialog({ projectId, onSuccess }: AgenticCrawlDialogProps) {
  // State management for URLs, prompt, loading
  // Form validation
  // API call to /crawl/agentic
  // Success feedback with Job ID
}
```

## ğŸ“Š PrzepÅ‚yw Danych

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User Input    â”‚
â”‚  URLs + Prompt  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Intelligent Engine Selector            â”‚
â”‚  - Analyze URLs (patterns, domains)     â”‚
â”‚  - Parse prompt (complexity, keywords)  â”‚
â”‚  - Confidence scoring                   â”‚
â”‚  - Engine recommendation                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Selected Engineâ”‚
â”‚  HTTP/Playwrightâ”‚
â”‚  /Firecrawl     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Content Scraping       â”‚
â”‚  - Multiple URLs        â”‚
â”‚  - Parallel processing  â”‚
â”‚  - Error handling       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AI-Guided Extraction    â”‚
â”‚  - Claude/OpenAI API     â”‚
â”‚  - Custom prompt         â”‚
â”‚  - Structured output     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Knowledge Organization        â”‚
â”‚  - Hierarchical categories     â”‚
â”‚  - Document creation           â”‚
â”‚  - Chunk generation            â”‚
â”‚  - Vector embeddings           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL DB  â”‚
â”‚  + pgvector     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ PrzykÅ‚ady UÅ¼ycia

### PrzykÅ‚ad 1: Ekstrakcja Firm z Katalogu

**URLs:**
```
https://firmy.example.com/listing/page-1
https://firmy.example.com/listing/page-2
https://firmy.example.com/listing/page-3
```

**Prompt:**
```
WyciÄ…gnij wszystkie firmy z nastÄ™pujÄ…cymi danymi:
- Nazwa firmy
- Adres (ulica, miasto, kod pocztowy)
- Telefon kontaktowy
- Email
- Strona internetowa
- Opis dziaÅ‚alnoÅ›ci

Zignoruj reklamy i nawigacjÄ™.
```

**Wynik:**
- System uÅ¼yje **HTTP scraper** (szybki, statyczne listy)
- AI wyekstrauje strukturalne dane zgodnie z promptem
- Powstanie dokument z hierarchicznym drzewem kategorii

### PrzykÅ‚ad 2: Analiza Konkurencji z SPA

**URLs:**
```
https://competitor-spa.com/products
https://competitor-spa.com/pricing
```

**Prompt:**
```
Zbierz informacje o:
- Wszystkie produkty z cenami i opisami
- Plany cenowe i ich funkcjonalnoÅ›ci
- Unikalne punkty sprzedaÅ¼y (USP)

PorÃ³wnaj ceny i funkcje.
```

**Wynik:**
- System uÅ¼yje **Playwright** (JavaScript-heavy SPA)
- Renderowanie w prawdziwej przeglÄ…darce
- Kompletna ekstrakcja dynamicznej treÅ›ci

### PrzykÅ‚ad 3: Transkrypcja YouTube

**URLs:**
```
https://youtube.com/watch?v=XYZ123
https://youtube.com/watch?v=ABC456
```

**Prompt:**
```
WyciÄ…gnij transkrypcjÄ™ filmÃ³w i stwÃ³rz artykuÅ‚ podsumowujÄ…cy:
- GÅ‚Ã³wne punkty i wnioski
- Kluczowe cytaty
- Praktyczne wskazÃ³wki

Struktura: wstÄ™p, gÅ‚Ã³wne tematy, podsumowanie.
```

**Wynik:**
- System rozpozna YouTube URLs
- YouTube transcriber API
- AI summary wedÅ‚ug promptu

## âš™ï¸ Konfiguracja

### Zmienne Åšrodowiskowe

```bash
# Backend (.env)
ANTHROPIC_API_KEY=sk-ant-xxx  # Claude API (rekomendowane dla AI extraction)
OPENAI_API_KEY=sk-xxx         # OpenAI gpt-4o-mini (primary LLM)
FIRECRAWL_API_KEY=fc-xxx      # Opcjonalny, premium scraping
```

### Limity i Ustawienia

```python
# backend/api/routes/crawl.py
MAX_URLS_AGENTIC_CRAWL = 20        # Max URLs per request
PROMPT_MIN_LENGTH = 10              # Min prompt characters
PROMPT_MAX_LENGTH = 1000            # Max prompt characters
CELERY_TASK_PRIORITY = 7            # Higher priority for agentic tasks
```

## ğŸ” Monitorowanie i Debugowanie

### Sprawdzanie Statusu Zadania

```bash
curl -X GET "http://localhost:8765/api/v1/crawl/jobs/42" \
  -H "Authorization: Bearer YOUR_TOKEN"
```

**OdpowiedÅº:**
```json
{
  "job_id": 42,
  "project_id": 1,
  "status": "in_progress",
  "total_urls": 3,
  "completed_urls": 2,
  "failed_urls": 0,
  "created_at": "2026-01-29T12:00:00Z",
  "updated_at": "2026-01-29T12:05:00Z"
}
```

### Logi Celery Worker

```bash
# Sprawdzenie wyboru silnika i przetwarzania
docker logs knowledgetree-celery-worker -f | grep -E "crawl|engine|agentic"
```

**PrzykÅ‚adowe Logi:**
```
[2026-01-29 12:05:00] INFO: Auto-selected engine: http
[2026-01-29 12:05:00] INFO: Intelligent selection for: wyciÄ…gnij wszystkie firmy...
[2026-01-29 12:05:01] INFO: Scraping 3 URLs with engine=http
[2026-01-29 12:05:03] INFO: Scraping complete: 3 succeeded, 0 failed
[2026-01-29 12:05:03] INFO: Extracting with custom prompt: wyciÄ…gnij wszystkie...
```

## ğŸš¨ ObsÅ‚uga BÅ‚Ä™dÃ³w

### Typowe BÅ‚Ä™dy i RozwiÄ…zania

**1. "Engine selection failed"**
- SprawdÅº dostÄ™pnoÅ›Ä‡ Playwright lub Firecrawl
- Fallback na HTTP scraper automatyczny

**2. "AI extraction failed"**
- SprawdÅº ANTHROPIC_API_KEY / OPENAI_API_KEY
- Prompt moÅ¼e byÄ‡ zbyt skomplikowany - uproÅ›Ä‡

**3. "URL scraping timeout"**
- ZwiÄ™ksz timeout w ustawieniach
- NiektÃ³re strony mogÄ… blokowaÄ‡ boty

**4. "Event loop error" (fixed w v1.1)**
- Update do najnowszej wersji
- Fix: uÅ¼ywamy `asyncio.run()` zamiast `new_event_loop()`

## ğŸ“ˆ WydajnoÅ›Ä‡

### Benchmarki

| Scenariusz | URLs | Czas | Silnik |
|------------|------|------|--------|
| Proste listy HTML | 10 | ~15s | HTTP |
| SPA z JS | 5 | ~45s | Playwright |
| YouTube | 3 | ~30s | YouTube API |
| Mix stron | 20 | ~2min | Auto |

### Optymalizacja

- **RÃ³wnolegÅ‚e przetwarzanie**: Do 5 URLs jednoczeÅ›nie
- **Caching**: Ponowne zapytania wykorzystujÄ… cache
- **Inteligentny fallback**: Automatyczne przeÅ‚Ä…czanie silnikÃ³w przy bÅ‚Ä™dach

## ğŸ” BezpieczeÅ„stwo

- **Autentykacja**: Bearer token wymagany
- **Rate limiting**: Zapobieganie naduÅ¼yciom
- **URL validation**: Walidacja formatÃ³w i domen
- **Sandbox execution**: Izolowane Å›rodowisko crawlingu

## ğŸ“ Changelog

### v1.1.0 (2026-01-29)
- âœ¨ Dodano inteligentny wybÃ³r silnika
- âœ¨ Agentic crawl z custom AI prompts
- ğŸ› Fix: Event loop error w Celery tasks
- ğŸ“š Dokumentacja w jÄ™zyku polskim

### v1.0.0 (2026-01-20)
- ğŸ‰ Podstawowy crawling HTTP/Playwright
- ğŸ‰ Single URL i batch crawling

## ğŸ¤ Wsparcie

- **GitHub Issues**: https://github.com/yourusername/knowledgetree/issues
- **Dokumentacja API**: http://localhost:8765/docs
- **Email**: support@knowledgetree.com

---

**Stworzone z â¤ï¸ dla KnowledgeTree**
