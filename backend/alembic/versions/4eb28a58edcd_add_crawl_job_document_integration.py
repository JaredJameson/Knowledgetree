"""add_crawl_job_document_integration

Revision ID: 4eb28a58edcd
Revises: 6193dfbb978c
Create Date: 2026-01-29 08:47:36.309861

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = '4eb28a58edcd'
down_revision: Union[str, Sequence[str], None] = '6193dfbb978c'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('categories', sa.Column('source_url', sa.Text(), nullable=True))
    op.add_column('categories', sa.Column('url_path', sa.Text(), nullable=True))
    op.add_column('categories', sa.Column('content_hash', sa.String(length=32), nullable=True))
    op.add_column('categories', sa.Column('last_crawled_at', sa.DateTime(timezone=True), nullable=True))
    op.add_column('categories', sa.Column('etag', sa.String(), nullable=True))
    op.add_column('categories', sa.Column('last_modified', sa.String(), nullable=True))
    op.add_column('crawl_jobs', sa.Column('document_id', sa.Integer(), nullable=True))
    op.add_column('crawl_jobs', sa.Column('max_depth', sa.Integer(), nullable=False, server_default='2'))
    op.add_column('crawl_jobs', sa.Column('url_patterns', postgresql.JSONB(astext_type=sa.Text()), nullable=True))
    op.add_column('crawl_jobs', sa.Column('content_filters', postgresql.JSONB(astext_type=sa.Text()), nullable=True))
    op.add_column('crawl_jobs', sa.Column('extraction_method', sa.String(length=50), nullable=False, server_default='auto'))
    op.add_column('crawl_jobs', sa.Column('last_crawl_at', sa.DateTime(timezone=True), nullable=True))
    op.create_index(op.f('ix_crawl_jobs_document_id'), 'crawl_jobs', ['document_id'], unique=False)
    op.create_foreign_key(None, 'crawl_jobs', 'documents', ['document_id'], ['id'], ondelete='SET NULL')
    op.add_column('documents', sa.Column('crawl_job_id', sa.Integer(), nullable=True))
    op.create_index(op.f('ix_documents_crawl_job_id'), 'documents', ['crawl_job_id'], unique=False)
    op.create_foreign_key(None, 'documents', 'crawl_jobs', ['crawl_job_id'], ['id'], ondelete='SET NULL')
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_constraint(None, 'documents', type_='foreignkey')
    op.drop_index(op.f('ix_documents_crawl_job_id'), table_name='documents')
    op.drop_column('documents', 'crawl_job_id')
    op.drop_constraint(None, 'crawl_jobs', type_='foreignkey')
    op.drop_index(op.f('ix_crawl_jobs_document_id'), table_name='crawl_jobs')
    op.drop_column('crawl_jobs', 'last_crawl_at')
    op.drop_column('crawl_jobs', 'extraction_method')
    op.drop_column('crawl_jobs', 'content_filters')
    op.drop_column('crawl_jobs', 'url_patterns')
    op.drop_column('crawl_jobs', 'max_depth')
    op.drop_column('crawl_jobs', 'document_id')
    op.drop_column('categories', 'last_modified')
    op.drop_column('categories', 'etag')
    op.drop_column('categories', 'last_crawled_at')
    op.drop_column('categories', 'content_hash')
    op.drop_column('categories', 'url_path')
    op.drop_column('categories', 'source_url')
    # ### end Alembic commands ###
